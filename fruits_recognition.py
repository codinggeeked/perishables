# -*- coding: utf-8 -*-
"""Fruits recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17b7M5h0PhYLu8xaJZWPpONRZ1oDC__-z

## Fruits recognition

### Import all the Dependencies
"""

import tensorflow as tf # Import TensorFlow library and specific modules
from tensorflow.keras import models, layers
import matplotlib.pyplot as plt # Import matplotlib for visualization

"""#### Set constant"""

BATCH_SIZE = 32  # Specify the batch size
IMAGE_SIZE = 280 # Specify the desired image size
CHANNELS=3 #Number of color channels in the images (3 for RGB)
EPOCHS=40 #Iterations through dataset

"""#### Import fruits data into tensorflow dataset object & Mount google drive to fetch file


"""

from google.colab import drive
drive.mount('/content/gdrive') #mounting google drive to pull resources from it
dataset_path = '/content/gdrive/MyDrive/AI/fruitsdataset' #path to the dataset

dataset = tf.keras.preprocessing.image_dataset_from_directory(
    '/content/gdrive/MyDrive/AI/fruitsdataset/train',

    seed=123,     # Specify a seed for reproducibility
    shuffle=True, # Whether to shuffle the dataset
    image_size=(IMAGE_SIZE,IMAGE_SIZE), #used as specified on the upper block
    batch_size=BATCH_SIZE               #used as specified on the upper block
)

class_names = dataset.class_names #Class names
class_names

for image_batch, labels_batch in dataset.take(1): # Loop over the first batch of the dataset
    print(image_batch.shape) # Print the shape of the image batch
    print(labels_batch.numpy()) # Print the corresponding labels batch in numpy format

"""Each element in the dataset is a tuple. First element is a batch of 32 elements of images. Second element is a batch of 32 elements of class labels

### Visualize some of the images from our dataset
"""

plt.figure(figsize=(10, 10)) #Size for Visualized Figures
for image_batch, labels_batch in dataset.take(1):
    for i in range(12):
        ax = plt.subplot(3, 4, i + 1) #Plotting 12 images in  3 rows by 4 columns format
        plt.imshow(image_batch[i].numpy().astype("uint8"))
        plt.title(class_names[labels_batch[i]]) #Titles of fruits
        plt.axis("off") #Axis hidden for clearer visuals

"""### Function to Split Dataset
Dataset should be bifurcated into 3 subsets, namely:
<br>
**Training**: Dataset to be used while training <br>
**Validation**: Dataset to be tested against while training<br>
**Test**: Dataset to be tested against after we trained a model<br>
"""

def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):
    assert (train_split + test_split + val_split) == 1 #dataset split, 80% for trainning and 10% each for validation & test

    ds_size = len(ds)

    if shuffle:
        ds = ds.shuffle(shuffle_size, seed=12)

    train_size = int(train_split * ds_size)
    val_size = int(val_split * ds_size)

    train_ds = ds.take(train_size)
    val_ds = ds.skip(train_size).take(val_size)
    test_ds = ds.skip(train_size).skip(val_size)

    return train_ds, val_ds, test_ds

train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset)

len(train_ds)

len(val_ds)

len(test_ds)

"""### Cache, Shuffle, and Prefetch the Dataset

## Cache, shuffle, and prefetch operations for training dataset

# Cache, shuffle, and prefetch operations for training dataset
"""

train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE) # Cache the dataset in memory for faster access during training
val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE) # Shuffle the dataset to introduce randomness during training
test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE) # Prefetch batches to overlap data preprocessing and model execution

"""### Building the Model"""



"""#### Creating a Layer for Resizing and Normalization

Before we feed our images to network, we resize it to the desired size.

This will be useful when we are done with the training and start using the model for predictions. At that time somone can supply an image that is not (256,256) and this layer will resize it
"""

resize_and_rescale = tf.keras.Sequential([
  layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),
  layers.experimental.preprocessing.Rescaling(1./255),
])

"""#### Data Augmentation

Data Augmentation is needed when we have less data, this boosts the accuracy of our model by augmenting the data.
Random rotation function is used.
"""

data_augmentation = tf.keras.Sequential([
  layers.experimental.preprocessing.RandomFlip("horizontal_and_vertical"),
  layers.experimental.preprocessing.RandomRotation(0.2),
])

"""#### Applying Data Augmentation to Train Dataset"""

train_ds = train_ds.map(
    lambda x, y: (data_augmentation(x, training=True), y)
).prefetch(buffer_size=tf.data.AUTOTUNE)

"""### Model Architecture

We use a CNN coupled with a Softmax activation in the output layer. We also add the initial layers for resizing, normalization and Data Augmentation.
"""

input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)
n_classes = 6

model = models.Sequential([
    resize_and_rescale,
    layers.Conv2D(32, kernel_size = (3,3), activation='relu', input_shape=input_shape),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(n_classes, activation='softmax'),
])

model.build(input_shape=input_shape)

model.summary()

"""#### Compiling the Model

We use adam Optimizer, SparseCategoricalCrossentropy for losses, accuracy as a metric
"""

model.compile(
    optimizer='adam',   # Use the Adam optimizer for gradient-based optimization
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), # Sparse categorical crossentropy loss function
    metrics=['accuracy'] # Track accuracy as a metric during training
)

history = model.fit(
    train_ds, # Training dataset
    batch_size=BATCH_SIZE, # Specify the batch size for training
    validation_data=val_ds, # Validation dataset for monitoring performance during training
    verbose=1, # Verbosity level (1: progress bar, 0: silent)
    epochs=40, # Number of training epochs..the number of iterations through dataset in training phase
)

scores = model.evaluate(test_ds)

"""we get 88% accuracy for our test dataset."""

scores

"""list containing loss and accuracy value

### Plotting the Accuracy and Loss Curves
"""

history

history.params

history.history.keys()

"""loss, accuracy, val loss etc are a python list containing values of loss, accuracy etc at the end of each epoch"""

type(history.history['loss'])

len(history.history['loss'])

history.history['loss'][:5] # show loss for first 5 epochs

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(range(EPOCHS), acc, label='Training Accuracy')
plt.plot(range(EPOCHS), val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(range(EPOCHS), loss, label='Training Loss')
plt.plot(range(EPOCHS), val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

"""#### Run prediction on a sample image"""

import numpy as np
for images_batch, labels_batch in test_ds.take(1):

    first_image = images_batch[0].numpy().astype('uint8')
    first_label = labels_batch[0].numpy()

    print("first image to predict")
    plt.imshow(first_image)
    print("actual label:",class_names[first_label])

    batch_prediction = model.predict(images_batch)
    print("predicted label:",class_names[np.argmax(batch_prediction[0])])

"""#### inference Function"""

def predict(model, img):
    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())
    img_array = tf.expand_dims(img_array, 0)

    predictions = model.predict(img_array)

    predicted_class = class_names[np.argmax(predictions[0])]
    confidence = round(100 * (np.max(predictions[0])), 2)
    return predicted_class, confidence

"""Running inference on few sample imagesNow run inference on few sample images"""

plt.figure(figsize=(15, 15))
for images, labels in test_ds.take(1):
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1) #3 rows by 3 columns
        plt.imshow(images[i].numpy().astype("uint8"))

        predicted_class, confidence = predict(model, images[i].numpy())
        actual_class = class_names[labels[i]]

        plt.title(f"Actual: {actual_class},\n Predicted: {predicted_class}.\n Confidence: {confidence}%")

        plt.axis("off")

"""#### Saving the Model"""

model.save('/content/gdrive/MyDrive/AI/perishables.ipynb')

